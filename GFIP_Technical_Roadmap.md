# GFIP è¯¦ç»†æŠ€æœ¯è·¯çº¿å›¾ (Gene Family Identification Pipeline)

> **ç‰ˆæœ¬**: v3.1 (Quad-Core Edition)  
> **æœ€åæ›´æ–°**: 2026-01-24  
> **æ–‡æ¡£ç›®çš„**: è¯¦ç»†è®°å½• Pipeline æ¯ä¸€æ­¥çš„æŠ€æœ¯ç»†èŠ‚ã€æ•°æ®æµã€å¤„ç†é€»è¾‘å’Œå…³é”®å‚æ•°

---

## ç›®å½•

1. [æ€»ä½“æ¶æ„](#æ€»ä½“æ¶æ„)
2. [Phase 1: ç§å­åºåˆ—è·å–](#phase-1-ç§å­åºåˆ—è·å–)
3. [Phase 2: HMMæ¨¡å‹æ„å»º](#phase-2-hmmæ¨¡å‹æ„å»º)
4. [Phase 3: å››æ ¸å¿ƒæœç´¢](#phase-3-å››æ ¸å¿ƒæœç´¢)
5. [Phase 4: åŸŸéªŒè¯](#phase-4-åŸŸéªŒè¯)
6. [Phase 5: åºåˆ—æå–](#phase-5-åºåˆ—æå–)
7. [Phase 6: å¤šåºåˆ—æ¯”å¯¹ä¸Motifåˆ†æ](#phase-6-å¤šåºåˆ—æ¯”å¯¹ä¸motifåˆ†æ)
8. [Phase 7: ç³»ç»Ÿå‘è‚²æ ‘æ„å»º](#phase-7-ç³»ç»Ÿå‘è‚²æ ‘æ„å»º)
9. [Phase 8: Ka/Ksé€‰æ‹©å‹åŠ›åˆ†æ](#phase-8-kaksé€‰æ‹©å‹åŠ›åˆ†æ)
10. [Phase 9: å…±çº¿æ€§åˆ†æ](#phase-9-å…±çº¿æ€§åˆ†æ)
11. [Phase 10: å¯åŠ¨å­åˆ†æ](#phase-10-å¯åŠ¨å­åˆ†æ)
12. [å…³é”®æŠ€æœ¯ç»†èŠ‚å¤‡å¿˜](#å…³é”®æŠ€æœ¯ç»†èŠ‚å¤‡å¿˜)

---

## æ€»ä½“æ¶æ„

```
ç”¨æˆ·è¾“å…¥ (query, domains, genome, proteome, cds, gff)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: ç§å­è·å– (retrieve_seeds.py)                                â”‚
â”‚   ä¸‰è½¨å¹¶è¡Œ: NCBI + UniProt + InterPro                                â”‚
â”‚   åˆ†ç±»è¾“å‡º: seeds_gold.fasta + seeds_broad.fasta                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: HMMæ„å»º (build_hmm.py)                                      â”‚
â”‚   å»é‡ â†’ é•¿åº¦è¿‡æ»¤ â†’ MAFFTæ¯”å¯¹ â†’ Gapä¿®å‰ª â†’ hmmbuild                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: å››æ ¸å¿ƒæœç´¢ (search_extract.py x 2 + BLAST x 2)              â”‚
â”‚   HMM-Gold + HMM-Broad + BLAST-Gold + BLAST-Broad                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: åŸŸéªŒè¯ (scan_cdd_ncbi.py + interproscan_runner.py)          â”‚
â”‚   CDD + InterPro â†’ Union/Intersection                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 5: åºåˆ—æå– (universal_family_extractor.py)                    â”‚
â”‚   æœ€é•¿å¼‚æ„ä½“è¿‡æ»¤ + IDæ˜ å°„ â†’ PEP/CDS/Gene/Promoter                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼                â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MSA     â”‚   â”‚ Ka/Ks     â”‚    â”‚ å…±çº¿æ€§     â”‚    â”‚ å¯åŠ¨å­     â”‚
â”‚ ç³»ç»Ÿå‘è‚² â”‚   â”‚ é€‰æ‹©å‹åŠ›   â”‚    â”‚ åˆ†æ      â”‚    â”‚ åˆ†æ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
HTMLæŠ¥å‘Š + å¯è§†åŒ–
```

---

## Phase 1: ç§å­åºåˆ—è·å–

### è„šæœ¬: `retrieve_seeds.py` (463è¡Œ)

#### 1.1 æ•°æ®æºä¸API

| æ•°æ®æº | å‡½æ•° | APIç«¯ç‚¹ | è®¤è¯ |
|--------|------|---------|------|
| **NCBI Protein** | `get_ncbi_many()` | Entrez ESearch + EFetch | API Key (å¯é€‰) |
| **UniProt** | `get_uniprot_seeds_by_name()` | `rest.uniprot.org/uniprotkb/search` | æ—  |
| **InterPro** | `get_interpro_seeds()` | `ebi.ac.uk/interpro/api` | æ—  |

#### 1.2 è¯·æ±‚é‡è¯•æœºåˆ¶

```python
# get_session() - åˆ›å»ºå¸¦é‡è¯•é€»è¾‘çš„Session
session = requests.Session()
retries = Retry(
    total=5,                # æœ€å¤šé‡è¯•5æ¬¡
    backoff_factor=1,       # 1s, 2s, 4s, 8s, 16s æŒ‡æ•°é€€é¿
    status_forcelist=[429, 500, 502, 503, 504],  # è¿™äº›çŠ¶æ€ç è§¦å‘é‡è¯•
    allowed_methods=["HEAD", "GET", "POST"]
)
```

#### 1.3 NCBIé€Ÿç‡é™åˆ¶å¤„ç†

```python
# å…³é”®å‚æ•°
batch_size = 400                    # æ¯æ‰¹ä¸‹è½½çš„åºåˆ—æ•°
max_workers = 3                     # å¹¶å‘çº¿ç¨‹æ•° (å‡å°‘ä»¥é¿å…429)
wait_time = 2 + random.uniform(0, 1)  # æŸ¥è¯¢é—´éš” (2-3ç§’)

# 429é”™è¯¯å¤„ç†
if resp.status_code == 429:
    time.sleep(10 + retry_count * 5 + random.uniform(0, 2))  # 10-30ç§’+
```

#### 1.4 ç§å­åˆ†ç±»é€»è¾‘ (Gold vs Silver)

```python
def classify_sequence(rec, source):
    """
    GOLD (é«˜å¯ä¿¡åº¦):
    â”œâ”€â”€ NCBI: NP_* æˆ– YP_* (å·²éªŒè¯çš„RefSeq)
    â”œâ”€â”€ UniProt: Swiss-Prot (reviewed:true)
    â””â”€â”€ InterPro: /protein/reviewed/ ç«¯ç‚¹

    SILVER (é¢„æµ‹/æœªå®¡æ ¸):
    â”œâ”€â”€ NCBI: XP_* (é¢„æµ‹è›‹ç™½), WP_* (éå†—ä½™)
    â”œâ”€â”€ UniProt: TrEMBL (reviewed:false)
    â””â”€â”€ InterPro: /protein/unreviewed/ ç«¯ç‚¹
    """
```

#### 1.5 InterProåˆ†æ‰¹ä¸‹è½½

```python
# fetch_interpro_endpoint() è¯¦ç»†æµç¨‹:
# 1. æ”¶é›†Accession IDs (åˆ†é¡µè¯·æ±‚)
while next_url and len(accessions) < limit:
    resp = session.get(next_url, headers={"Accept": "application/json"})
    for res in data["results"]:
        accessions.append(res["metadata"]["accession"])
    next_url = data.get("next")

# 2. æ‰¹é‡ä¸‹è½½åºåˆ—
batch_size = 100
max_workers = 20  # é«˜å¹¶å‘ä¸‹è½½
for batch in batches:
    url = "https://rest.uniprot.org/uniprotkb/accessions"
    params = {"accessions": ",".join(batch), "format": "fasta"}
```

#### 1.6 å»é‡ä¸è¾“å‡º

```python
# åŸºäºåºåˆ—å†…å®¹å»é‡ (éID)
seen = set()
for rec in all_records:
    seq_str = str(rec.seq).upper()
    if seq_str in seen: continue
    seen.add(seq_str)
    
    # æ ¹æ®åˆ†ç±»æ ‡ç­¾åˆ†é…
    if rec._classification == "GOLD":
        gold_recs.append(rec)
    else:
        silver_recs.append(rec)

# è¾“å‡ºæ–‡ä»¶:
# - {family}_seeds_gold.fasta: ä»…Goldåºåˆ—
# - {family}_seeds_broad.fasta: Gold + Silver (å—max_seedsé™åˆ¶)
```

---

## Phase 2: HMMæ¨¡å‹æ„å»º

### è„šæœ¬: `build_hmm.py` (212è¡Œ)

#### 2.1 åºåˆ—åˆå¹¶ä¸å»é‡

```python
def merge_and_deduplicate(input_files, output_file):
    seen_sequences = set()
    unique_records = []
    
    for record in SeqIO.parse(file_path, "fasta"):
        seq_str = str(record.seq).strip().upper()
        if seq_str not in seen_sequences:
            seen_sequences.add(seq_str)
            unique_records.append(record)
```

#### 2.2 âš ï¸ é•¿åº¦è¿‡æ»¤ (å…³é”®æ­¥éª¤)

```python
# åŸºäºä¸­ä½æ•°é•¿åº¦è¿‡æ»¤å¼‚å¸¸å€¼
lengths = [len(r.seq) for r in unique_records]
lengths.sort()
median_len = lengths[len(lengths)//2]

# ä¿ç•™èŒƒå›´: ä¸­ä½æ•°çš„ 75% ~ 125%
min_len = int(median_len * 0.75)
max_len = int(median_len * 1.25)

filtered = [r for r in unique_records if min_len <= len(r.seq) <= max_len]

# ç¤ºä¾‹: ä¸­ä½æ•°=500aa â†’ ä¿ç•™375-625aaçš„åºåˆ—
```

#### 2.3 MAFFTå¤šåºåˆ—æ¯”å¯¹

```bash
# å®é™…æ‰§è¡Œå‘½ä»¤
mafft --auto --thread {cpu} seeds_merged.fasta > seeds_aligned.fasta

# --auto: è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç®—æ³•
#   - <200æ¡: L-INS-i (æœ€å‡†ç¡®)
#   - 200-2000æ¡: FFT-NS-i
#   - >2000æ¡: FFT-NS-2 (æœ€å¿«)
```

#### 2.4 âš ï¸ MSA Gapä¿®å‰ª (HMMæ„å»ºå‰)

```python
def clean_msa(input_file, output_file, max_gap_fraction=0.9):
    """
    ç§»é™¤Gapæ¯”ä¾‹>90%çš„åˆ—
    (æ³¨æ„: è¿™é‡Œç”¨90%æ˜¯ä¸ºHMMæ„å»ºä¼˜åŒ–, ç³»ç»Ÿå‘è‚²æ ‘ç”¨50%)
    """
    for i in range(seq_len):
        col = [rec.seq[i] for rec in alignment]
        gap_count = col.count("-")
        if gap_count / len(alignment) <= max_gap_fraction:
            keep_cols.append(i)
```

#### 2.5 hmmbuildæ„å»ºHMM

```bash
# å®é™…æ‰§è¡Œå‘½ä»¤
hmmbuild -n {family_name} family.hmm seeds_aligned.fasta

# è¾“å‡º: family_gold.hmm, family_broad.hmm
```

---

## Phase 3: å››æ ¸å¿ƒæœç´¢

### è„šæœ¬: `search_extract.py` (140è¡Œ)

#### 3.1 å››æ¡æœç´¢æµé…ç½®

| æµ | æ¨¡å‹/ç§å­ | å·¥å…· | è¾“å‡ºæ–‡ä»¶ |
|----|-----------|------|----------|
| **Stream 1** | `family_gold.hmm` | hmmsearch | `hits_hmm_gold.fasta` |
| **Stream 2** | `family_broad.hmm` | hmmsearch | `hits_hmm_broad.fasta` |
| **Stream 3** | `seeds_gold.fasta` | BLASTp | `hits_blast_gold.fasta` |
| **Stream 4** | `seeds_broad.fasta` | BLASTp | `hits_blast_broad.fasta` |

#### 3.2 hmmsearchæ‰§è¡Œç»†èŠ‚

```python
def run_hmmsearch(hmm_file, target_proteome, output_tbl, threads=4, evalue=1e-5):
    cmd = [
        "hmmsearch",
        "--tblout", output_tbl,     # è¡¨æ ¼è¾“å‡º
        "--cpu", str(threads),
        "-E", str(evalue),          # E-valueé˜ˆå€¼
        hmm_file,
        target_proteome
    ]
    # stdouté‡å®šå‘åˆ°DEVNULLä»¥é¿å…å¤§é‡æ¯”å¯¹è¾“å‡º
    subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True)
```

#### 3.3 ç»“æœè§£æä¸åºåˆ—æå–

```python
def parse_and_extract(tbl_file, target_proteome, output_fasta, evalue_cutoff):
    valid_ids = set()
    
    with open(tbl_file, 'r') as f:
        for line in f:
            if line.startswith("#"): continue
            parts = line.split()
            
            target_name = parts[0]  # åˆ—0: ç›®æ ‡åºåˆ—å
            evalue = float(parts[4])  # åˆ—4: Full sequence E-value
            
            if evalue <= evalue_cutoff:
                valid_ids.add(target_name)
    
    # ä½¿ç”¨ç´¢å¼•æå–åºåˆ— (å†…å­˜é«˜æ•ˆ)
    proteome_dict = SeqIO.index(target_proteome, "fasta")
    for pid in valid_ids:
        if pid in proteome_dict:
            extracted_records.append(proteome_dict[pid])
```

#### 3.4 å€™é€‰åˆå¹¶

```python
# run_pipeline_v3.py ä¸­çš„åˆå¹¶é€»è¾‘
hits_files = [hmm_gold, hmm_broad, blast_gold, blast_broad]
seen_ids = set()

for hit_file in hits_files:
    if os.path.exists(hit_file):
        for rec in SeqIO.parse(hit_file, "fasta"):
            if rec.id not in seen_ids:
                seen_ids.add(rec.id)
                all_candidates.append(rec)
```

---

## Phase 4: åŸŸéªŒè¯

### 4.1 CDDæ‰«æ

#### è„šæœ¬: `scan_cdd_ncbi.py` (191è¡Œ)

##### 4.1.1 æäº¤æµç¨‹

```python
def submit_search(fasta_file, db="cdd", evalue=0.01):
    # åºåˆ—æ¸…æ´—: ç§»é™¤ç»ˆæ­¢å¯†ç å­ç¬¦å·
    clean_seq = str(seq.seq).replace("*", "")
    
    payload = {
        "db": db,           # cdd, pfam, smart ç­‰
        "smode": "auto",    # è‡ªåŠ¨æ¨¡å¼é€‰æ‹©
        "useid1": "true",   # ä½¿ç”¨ç”¨æˆ·æä¾›çš„ID
        "filter": "true",   # ä½å¤æ‚åº¦è¿‡æ»¤
        "evalue": str(evalue),
        "tdata": "hits",    # å‘½ä¸­è¡¨æ ¼è¾“å‡º
        "queries": query_str
    }
    
    resp = requests.post(CDSEARCH_URL, data=payload, timeout=30)
    
    # ä»å“åº”ä¸­æå–Job ID (å¤šç§æ ¼å¼)
    # æ ¼å¼1: Search-ID: QM3-xxx
    # æ ¼å¼2: value="QM3-xxx" name="cdsid"
    # æ ¼å¼3: #cdsid QM3-xxx
```

##### 4.1.2 âš ï¸ æŒ‡æ•°é€€é¿é‡è¯•

```python
# æäº¤å¤±è´¥æ—¶çš„é‡è¯•ç­–ç•¥
max_retries = 5
for attempt in range(max_retries):
    cdsid = submit_search(args.input, args.db, args.evalue)
    
    if cdsid: break
    
    # æŒ‡æ•°é€€é¿: 30s, 60s, 120s, 240s, 480s + æŠ–åŠ¨
    wait_time = 30 * (2 ** attempt) + random.uniform(-5, 5)
    wait_time = max(10, wait_time)  # æœ€å°‘10ç§’
    time.sleep(wait_time)
```

##### 4.1.3 ç»“æœè½®è¯¢

```python
def retrieve_results(cdsid, output_file):
    while True:
        resp = requests.post(CDSEARCH_URL, data={"cdsid": cdsid, "tdata": "hits"})
        
        # æ£€æŸ¥æ˜¯å¦å®Œæˆ (åŒ…å«è¡¨å¤´)
        if "Q#" in content and "Hit type" in content:
            # ç»“æœå°±ç»ª
            with open(output_file, "w") as f:
                f.write(content)
            return True
            
        time.sleep(10)  # æ¯10ç§’è½®è¯¢ä¸€æ¬¡
```

### 4.2 InterProæ‰«æ

#### è„šæœ¬: `interproscan_runner.py` (345è¡Œ)

##### 4.2.1 ä¸¤ç§è¿è¡Œæ¨¡å¼

| æ¨¡å¼ | å‚æ•° | è¯´æ˜ |
|------|------|------|
| **APIæ¨¡å¼** | é»˜è®¤ | ä½¿ç”¨EBI REST API |
| **æœ¬åœ°æ¨¡å¼** | `--local_path` | ä½¿ç”¨æœ¬åœ°interproscan.sh |

##### 4.2.2 âš ï¸ åºåˆ—æ¸…æ´— (å…³é”®æ­¥éª¤)

```python
# InterProScanå¯¹è¾“å…¥åºåˆ—éå¸¸æ•æ„Ÿ
# é—®é¢˜å­—ç¬¦ä¼šå¯¼è‡´æäº¤å¤±è´¥

# å…¨å±€æ¸…æ´— (run_robust_pipeline)
for s in sequences:
    # 1. ç§»é™¤ç»ˆæ­¢å¯†ç å­ (*)
    # 2. ç§»é™¤Gapå­—ç¬¦ (.)
    # 3. å»é™¤ç©ºç™½
    cleaned_seq = str(s.seq).upper().replace('*', '').replace('.', '').strip()
    
    # 4. è¿‡æ»¤çŸ­åºåˆ—
    if len(cleaned_seq) > 10:
        s.seq = Seq(cleaned_seq)  # é‡è¦: å¿…é¡»ç”¨Seqå¯¹è±¡!
        clean_sequences.append(s)
```

##### 4.2.3 åˆ†æ‰¹æäº¤é…ç½®

```python
MAX_CONCURRENT_JOBS = 5   # åŒæ—¶è¿è¡Œçš„Jobæ•°
batch_size = 20           # æ¯æ‰¹åºåˆ—æ•°

# ç”³è¯·çš„åˆ†æç±»å‹
appl = "PfamA,SuperFamily,CDD,Phobius,TMHMM,SignalP_EUK"
```

##### 4.2.4 JobçŠ¶æ€è½®è¯¢

```python
def process_batch(batch_data):
    job_id = submit_batch(batch_seqs, idx, email)
    
    # ç­‰å¾…å®Œæˆ (æ”¯æŒæ‰€æœ‰ç­‰å¾…çŠ¶æ€)
    while status in ["RUNNING", "QUEUED", "STARTED", "PENDING"]:
        time.sleep(10)
        status = check_status(job_id)
        
        if wait_count > 180:  # æœ€é•¿ç­‰å¾…30åˆ†é’Ÿ
            break
```

##### 4.2.5 æ–­ç‚¹ç»­ä¼  (Checkpoint)

```python
# æ¯æ‰¹å®Œæˆåç«‹å³ä¿å­˜
batch_file = os.path.join(temp_dir, f"batch_{idx}.tsv")

if os.path.exists(batch_file) and os.path.getsize(batch_file) > 0:
    # è·³è¿‡å·²å®Œæˆçš„æ‰¹æ¬¡
    print(f"Batch {idx} already exists. Skipping.")
    return cached_content
```

---

## Phase 5: åºåˆ—æå–

### è„šæœ¬: `universal_family_extractor.py` (802è¡Œ)

#### 5.1 IDå½’ä¸€åŒ–

```python
def normalize_id(pid):
    """
    å°†å„å¹³å°IDè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼:
    - XP_028218932.1 â†’ XP_028218932_1
    - cds-CAA33989.1 â†’ cds-CAA33989_1
    - EVM0001234.1 â†’ EVM0001234_1
    
    è§„åˆ™: å°† . å’Œ _ ç»Ÿä¸€è½¬ä¸º _
    """
    return re.sub(r'[._]', '_', pid.split()[0])
```

#### 5.2 âš ï¸ CDSæ˜ å°„ç­–ç•¥ (7ç§ç­–ç•¥)

```python
def build_cds_map(cds_path, proteome_path=None):
    """
    æ„å»º Protein ID â†’ CDS Record ID çš„æ˜ å°„
    """
    
    # === Step 1: Ensemblç‰¹æ®Šå¤„ç† (ä»è›‹ç™½æ–‡ä»¶æå–transcripté“¾æ¥) ===
    if proteome_path:
        for record in SeqIO.parse(proteome_path, "fasta"):
            # Ensemblè›‹ç™½: >cds-CAA33989.1 pep ... transcript:transcript-rps2 ...
            m = re.search(r'transcript:(\S+)', desc)
            if m:
                prot_to_transcript[pid] = m.group(1)
    
    # === Step 2: ä»CDSæ–‡ä»¶æ„å»ºæ˜ å°„ ===
    for record in SeqIO.parse(cds_path, "fasta"):
        candidates = []
        
        # Strategy 1: NCBI [protein_id=XP_123.1]
        m = re.search(r'\[protein_id=([^\]]+)\]', desc)
        if m: candidates.append(m.group(1))
        
        # Strategy 2: NCBI lcl|..._prot_XP_123.1
        if '_prot_' in rid:
            m2 = re.search(r'([XY]P_\d+\.\d+)', sub)
            
        # Strategy 3: NCBI lcl|..._cds_XP_123.1
        if '_cds_' in rid:
            m3 = re.search(r'([XY]P_\d+[._]\d+)', sub)
        
        # Strategy 4: Ensembl gene:xxx
        m4 = re.search(r'gene:(\S+)', desc)
        
        # Strategy 5: EVM gene=xxx
        m5 = re.search(r'gene=(\S+)', desc)
        
        # Strategy 6: ç›´æ¥ä½¿ç”¨record ID (ç®€å•æ ¼å¼å¦‚è¿ç¿˜)
        candidates.append(rid)
        
        # Strategy 7: ç¬¬ä¸€ä¸ªtoken
        first_token = desc.split()[0]
        candidates.append(first_token)
    
    # === Step 3: é€šè¿‡transcripté“¾æ¥Ensembl ===
    for prot_id, transcript_id in prot_to_transcript.items():
        if transcript_id in cds_records:
            mapping[prot_id] = cds_records[transcript_id]
```

#### 5.3 æœ€é•¿å¼‚æ„ä½“è¿‡æ»¤

```python
def filter_longest_isoforms(candidate_ids, proteome_path, gff_path):
    """
    æŒ‰Geneåˆ†ç»„, æ¯ç»„ä¿ç•™æœ€é•¿è½¬å½•æœ¬
    
    æ­¥éª¤:
    1. è§£æGFFè·å– mRNAâ†’Gene å…³ç³»
    2. è·å–æ¯ä¸ªå€™é€‰çš„è›‹ç™½é•¿åº¦
    3. æŒ‰Gene IDåˆ†ç»„
    4. æ¯ç»„é€‰æ‹©æœ€é•¿çš„
    """
    # æ„å»º mRNA â†’ Gene æ˜ å°„
    mrna_to_gene = {}
    for line in gff:
        if ftype == "mRNA":
            parent = attrs.get("Parent", "")
            mrna_to_gene[mrna_id] = parent
    
    # åˆ†ç»„
    gene_to_candidates = defaultdict(list)
    for cid in candidate_ids:
        gene = resolve_gene(cid)
        gene_to_candidates[gene].append((cid, length))
    
    # é€‰æœ€é•¿
    final_ids = []
    for gene, candidates in gene_to_candidates.items():
        candidates.sort(key=lambda x: x[1], reverse=True)
        final_ids.append(candidates[0][0])
```

#### 5.4 è¾“å‡ºæ–‡ä»¶

| æ–‡ä»¶ | å†…å®¹ | æå–é€»è¾‘ |
|------|------|----------|
| `family_members.pep.fasta` | è›‹ç™½åºåˆ— | ç›´æ¥ä»proteomeæå– |
| `family_members.cds.fasta` | CDSåºåˆ— | é€šè¿‡IDæ˜ å°„æå– |
| `family_members.gene.fasta` | åŸºå› ç»„åºåˆ— | ä»GFFè·å–åæ ‡ + ä»genomeæå– |
| `family_members.promoter.fasta` | å¯åŠ¨å­åºåˆ— | TSSä¸Šæ¸¸2000bp |
| `family_members.gff3` | åŸºå› ç»“æ„ | ä»åŸå§‹GFFè¿‡æ»¤ |

---

## Phase 6: å¤šåºåˆ—æ¯”å¯¹ä¸Motifåˆ†æ

### 6.1 MAFFTæ¯”å¯¹

```python
# pipeline_utils.run_mafft_alignment()
cmd = ["mafft", "--auto", "--amino", "--thread", str(threads)]
# è¾“å…¥: family_members.pep.fasta
# è¾“å‡º: family_members.aln.fasta
```

### 6.2 âš ï¸ MSA Gapä¿®å‰ª (ç³»ç»Ÿå‘è‚²æ ‘å‰)

```python
def trim_msa_by_gap(input_aln, output_aln, max_gap_ratio=0.5):
    """
    ç§»é™¤Gapæ¯”ä¾‹>50%çš„åˆ— (æ¯”HMMæ„å»ºæ›´ä¸¥æ ¼)
    
    ç›®çš„:
    - æé«˜ç³»ç»Ÿå‘è‚²æ ‘æ‹“æ‰‘è´¨é‡
    - å‡å°‘æ¯”å¯¹å™ªéŸ³
    
    è¾“å‡º: family_members.trimmed.aln
    """
    for i in range(length):
        col = alignment[:, i]
        gap_count = col.count("-")
        gap_ratio = gap_count / len(col)
        
        if gap_ratio <= max_gap_ratio:  # <=50%
            keep_cols.append(i)
```

### 6.3 Motifåˆ†æ (ä¸¤ç§ç­–ç•¥)

#### ç­–ç•¥1: MEME (Gold Standard)

```python
# ä¼˜å…ˆå°è¯•æœ¬åœ°MEME
cmd = [
    "meme", input_fasta,
    "-o", out_dir,
    "-protein",
    "-nmotifs", str(n_motifs),  # é»˜è®¤15
    "-minw", "6",               # æœ€å°motifå®½åº¦
    "-maxw", "50",              # æœ€å¤§motifå®½åº¦
    "-mod", "zoops"             # Zero Or One Per Sequence
]

# Dockerå¤‡é€‰
if not local_meme:
    docker run memesuite/memesuite meme ...
```

#### ç­–ç•¥2: MSA-Based Fallback

```python
def extract_motifs_from_msa(msa_file, min_len=6, conservation_threshold=0.7):
    """
    ä»MSAç›´æ¥æå–ä¿å®ˆåŒºå—
    
    æ­¥éª¤:
    1. è®¡ç®—æ¯åˆ—ä¿å®ˆæ€§ (æœ€é«˜é¢‘ç‡æ°¨åŸºé…¸å æ¯”)
    2. ä¿å®ˆæ€§ >= 70% çš„åˆ—æ ‡è®°ä¸ºä¿å®ˆ
    3. è¿ç»­ä¿å®ˆåˆ—ç»„æˆåŒºå—
    4. åˆå¹¶è·ç¦»<=10çš„åŒºå—
    5. è¿‡æ»¤é•¿åº¦<6çš„åŒºå—
    """
    # ä¿å®ˆæ€§è®¡ç®—
    for i in range(aln_len):
        col_nogap = [c for c in col if c != '-']
        top_char, count = Counter(col_nogap).most_common(1)[0]
        freq = count / num_seqs
        conserved_cols.append(freq >= conservation_threshold)
    
    # åŒºå—åˆå¹¶
    for next_m in motifs_ranges[1:]:
        dist = next_m[0] - curr_m[1]
        if dist <= 10:  # åˆå¹¶è·ç¦»é˜ˆå€¼
            curr_m = (curr_m[0], next_m[1])
```

---

## Phase 7: ç³»ç»Ÿå‘è‚²æ ‘æ„å»º

### 7.1 å·¥å…·ä¼˜å…ˆçº§

```python
# run_pipeline_v3.py
iqtree_bin = shutil.which("iqtree2") or shutil.which("iqtree")

if iqtree_bin:
    run_iqtree(tree_input_aln, prefix, threads=cpu)
elif shutil.which("fasttree"):
    run_fasttree(tree_input_aln, tree_file, threads=cpu)
```

### 7.2 IQ-TREEå‚æ•°

```python
def run_iqtree(aln_file, out_prefix, threads=4):
    cmd = [
        iqtree_bin,
        "-s", aln_file,
        "-m", "MFP",       # ModelFinder Plus: è‡ªåŠ¨æ¨¡å‹é€‰æ‹©
        "-bb", "1000",     # 1000æ¬¡UFBoot
        "-alrt", "1000",   # 1000æ¬¡SH-aLRT
        "-nt", str(threads),
        "-redo"            # è¦†ç›–å·²æœ‰ç»“æœ
    ]
```

### 7.3 FastTreeå‚æ•°

```bash
fasttree -lg < trimmed.aln > tree.nwk
# -lg: LGè›‹ç™½æ¨¡å‹
```

---

## Phase 8: Ka/Ksé€‰æ‹©å‹åŠ›åˆ†æ

### è„šæœ¬: `run_kaks_analysis.py` (386è¡Œ)

#### 8.1 å®Œæ•´æµç¨‹

```
è›‹ç™½MSA (family_members.aln.fasta)
    â”‚
    â–¼ + CDS (family_members.cds.fasta)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: å¯†ç å­æ¯”å¯¹ (protein_to_codon_alignment)     â”‚
â”‚   è›‹ç™½Gap â†’ '---'                                   â”‚
â”‚   è›‹ç™½æ®‹åŸº â†’ å¯¹åº”å¯†ç å­                              â”‚
â”‚   ç§»é™¤: ç»ˆæ­¢å¯†ç å­, Nç¢±åŸº, Gapåˆ—                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: AXTæ ¼å¼è½¬æ¢ (write_axt_format)              â”‚
â”‚   ç”Ÿæˆæ‰€æœ‰é…å¯¹ç»„åˆ (All-vs-All)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: KaKs_Calculator                             â”‚
â”‚   æ–¹æ³•: MA (Model Averaging) é»˜è®¤                   â”‚
â”‚   æ”¯æŒ: YN (Yang-Nielsen), ML (Maximum Likelihood)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
å¯è§†åŒ– (ç›´æ–¹å›¾ + æ•£ç‚¹å›¾)
```

#### 8.2 âš ï¸ å¯†ç å­æ¯”å¯¹å‡½æ•°

```python
def protein_to_codon_alignment(protein_aln_file, cds_file, output_file, remove_gaps=True):
    """
    å°†è›‹ç™½MSA"ç¿»è¯‘"å›å¯†ç å­æ¯”å¯¹
    
    å…³é”®æ£€æŸ¥:
    1. é•¿åº¦ä¸€è‡´æ€§: |CDS_len - Prot_len*3| <= 3
    2. ç§»é™¤: ç»ˆæ­¢å¯†ç å­ (TAA, TAG, TGA)
    3. ç§»é™¤: åŒ…å«Nçš„å¯†ç å­
    4. ç§»é™¤: Gapåˆ— (å¦‚æœremove_gaps=True)
    """
    # æ˜ å°„é€»è¾‘
    for aa in pseq:
        if aa == "-":
            mapped_codons.append("---")
        else:
            codon = cseq[cds_idx : cds_idx+3]
            mapped_codons.append(codon)
            cds_idx += 3
    
    # è¿‡æ»¤åˆ—
    stops = {"TAA", "TAG", "TGA"}
    for i in range(aln_len):
        for rec in codon_aln_records:
            codon = rec['codons'][i]
            
            # æ£€æŸ¥Gap
            if remove_gaps and codon == '---':
                keep = False
            
            # æ£€æŸ¥Nç¢±åŸº
            if 'N' in codon:
                keep = False
            
            # æ£€æŸ¥ç»ˆæ­¢å¯†ç å­
            if codon.upper() in stops:
                keep = False
```

#### 8.3 AXTæ ¼å¼è½¬æ¢

```python
def write_axt_format(codon_aln_file, output_axt):
    """
    ç”ŸæˆAll-vs-Allé…å¯¹çš„AXTæ ¼å¼
    
    æ ¼å¼:
    Seq1&Seq2
    ATGCATGC...
    ATGCATGC...
    
    (ç©ºè¡Œåˆ†éš”)
    """
    from itertools import combinations
    
    for rec1, rec2 in combinations(records, 2):
        f.write(f"{rec1.id}&{rec2.id}\n")
        f.write(f"{str(rec1.seq)}\n")
        f.write(f"{str(rec2.seq)}\n")
        f.write("\n")
```

#### 8.4 å¹¶è¡ŒKaKsè®¡ç®—

```python
# åˆ†å—ç­–ç•¥
if n_threads > 1:
    chunk_size = math.ceil(total_pairs / n_threads)
    
    for i in range(n_threads):
        chunk_pairs = pairs[start:end]
        
        # å†™å…¥ä¸´æ—¶AXT
        with open(chunk_axt, 'w') as f:
            for block in chunk_pairs:
                f.writelines(block)
        
        # æäº¤è®¡ç®—
        cmd = ["KaKs_Calculator", "-i", chunk_axt, "-o", chunk_out, "-m", "MA"]
        executor.submit(run_cmd, cmd)

# åˆå¹¶ç»“æœ (ä¿ç•™ä¸€ä¸ªheader)
with open(kaks_out_file, 'w') as outfile:
    header_written = False
    for chunk_out in chunk_files:
        lines = f.readlines()
        if not header_written:
            outfile.write(lines[0])
            header_written = True
        outfile.writelines(lines[1:])
```

---

## Phase 9: å…±çº¿æ€§åˆ†æ

### è„šæœ¬: `run_synteny_analysis.py` (819è¡Œ)

#### 9.1 SyntenyAnalyzerç±»æ¦‚è§ˆ

```python
class SyntenyAnalyzer:
    def __init__(self, genome, gff, out_dir, prefix="species", 
                 threads=4, evalue="1e-5", cds=None, pep=None, highlights=None):
        """
        å‚æ•°:
        - genome: åŸºå› ç»„FASTAæ–‡ä»¶
        - gff: GFF3æ³¨é‡Šæ–‡ä»¶
        - cds/pep: å¯é€‰çš„CDS/è›‹ç™½åºåˆ— (å¦åˆ™ç”¨gffreadæå–)
        - highlights: é«˜äº®åŸºå› åˆ—è¡¨ (å®¶æ—æˆå‘˜ID)
        """
        self.work_dir = os.path.join(self.out_dir, "Synteny_Work")
        
        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.bed_file = f"{prefix}.bed"
        self.blast_file = f"{prefix}.{prefix}.last"
        self.anchors_file = f"{prefix}.{prefix}.anchors"
```

#### 9.2 Step 1: æ•°æ®å‡†å¤‡ (prepare_data)

##### 9.2.1 GFFâ†’BEDè½¬æ¢ (åŒéæ‰«æç­–ç•¥)

```python
def _fallback_gff_to_bed(self):
    """
    Pass 1: æ„å»ºå®Œæ•´æ˜ å°„
    """
    parent_map = {}       # FeatureID â†’ ParentID
    gene_attr_map = {}    # FeatureID â†’ GeneName
    feat_to_prot = {}     # FeatureID â†’ ProteinID
    feat_coords = {}      # FeatureID â†’ (chrom, start, end, strand)
    
    # éœ€è¦æå–çš„å±æ€§é”®
    target_keys = {'protein_id', 'id', 'name', 'alias', 'product', 'transcript_id'}
    gene_keys = {'gene', 'gene_id', 'locus_tag', 'name', 'gene_name'}
    
    for line in gff_file:
        # è§£æGFF9åˆ—æ ¼å¼
        parts = line.split('\t')
        ftype = parts[2]  # gene, mRNA, CDSç­‰
        
        for chunk in attr.split(';'):
            k, v = chunk.split('=', 1)
            
            if k == 'ID': feat_id = v
            elif k == 'Parent': parent = v
            elif k in gene_keys: found_gene_attrs[k] = v
            elif k in target_keys: potential_prot_ids.append(v)
            elif k == 'Dbxref':
                # è§£æ Dbxref=GeneID:xxx,Genbank:xxx
                for ref in v.split(','):
                    potential_prot_ids.append(ref.split(':')[-1])
    
    """
    Pass 2: è¾“å‡ºBED
    """
    target_types = {'mrna', 'transcript', 'cds'}
    
    for feature in gff_lines:
        if ftype not in target_types: continue
        
        # è§£æProteinID (ä¼˜å…ˆçº§)
        prot_id = feat_to_prot.get(feat_id)
        if not prot_id and parent:
            prot_id = feat_to_prot.get(parent)
        
        # BEDæ ¼å¼ (0-based start)
        bed_start = start - 1
        fout.write(f"{chrom}\t{bed_start}\t{end}\t{prot_id}\t1000\t{strand}\n")
```

##### 9.2.2 FASTAæ¸…æ´—

```python
def _clean_fasta(self, fasta_path):
    """
    JCVI/Diamondå…¼å®¹æ€§å¤„ç†:
    1. ä»…ä¿ç•™ID (ç§»é™¤description)
    2. ç§»é™¤ç»ˆæ­¢å¯†ç å­ (*)
    3. ç§»é™¤Gapå­—ç¬¦ (.)
    """
    for record in SeqIO.parse(fin, "fasta"):
        clean_id = record.id.split()[0]
        clean_seq = str(record.seq).replace('.', '').replace('*', '')
        fout.write(f">{clean_id}\n{clean_seq}\n")
```

#### 9.3 Step 2: åŒæºæœç´¢ (run_homology_search)

```python
def run_homology_search(self):
    # å·¥å…·ä¼˜å…ˆçº§: Diamond > BLAST+
    
    if check_dependency("diamond"):
        # Diamond (æ¯”BLASTå¿«100x)
        db_file = f"{prefix}.dmnd"
        
        # 1. æ„å»ºç´¢å¼•
        cmd_makedb = f"diamond makedb --in {pep_file} --db {db_file}"
        
        # 2. All-vs-Allæœç´¢
        # è¾“å‡ºæ ¼å¼: qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore
        cmd_blastp = f"""diamond blastp 
            -q {pep_file} 
            --db {db_file} 
            -o {blast_file} 
            --evalue {evalue} 
            --threads {threads}"""
    
    elif check_dependency("blastp"):
        # BLAST+ (è¾ƒæ…¢ä½†æ›´å‡†ç¡®)
        cmd_makedb = f"makeblastdb -in {pep_file} -dbtype prot -out {db_base}"
        cmd_blastp = f"blastp -query {pep_file} -db {db_base} -out {blast_file} -evalue {evalue} -num_threads {threads} -outfmt 6"
```

#### 9.4 Step 3: MCScanXå…±çº¿æ€§æ£€æµ‹ (run_synteny)

```python
def run_synteny(self):
    # åˆ‡æ¢åˆ°å·¥ä½œç›®å½• (JCVIè¦æ±‚)
    os.chdir(self.work_dir)
    
    # JCVI MCScanXå‘½ä»¤
    # --cscore=.7: å…±çº¿æ€§è¯„åˆ†é˜ˆå€¼
    # --no_strip_names: ä¿ç•™åŸå§‹ID (é‡è¦!)
    cmd = f"python -m jcvi.compara.catalog ortholog {prefix} {prefix} --cscore=.7 --no_strip_names"
    
    # è¾“å‡º: {prefix}.{prefix}.anchors (å…±çº¿æ€§åŸºå› å¯¹)
```

#### 9.5 Step 4: Circoså¯è§†åŒ–è¾“å…¥ç”Ÿæˆ (generate_circos_conf)

```python
def generate_circos_conf(self):
    # 1. è§£æBEDè·å–æŸ“è‰²ä½“å°ºå¯¸å’ŒåŸºå› åæ ‡
    for line in bed_file:
        chrom, start, end, name = parts[0:4]
        gene_coords[name] = (chrom, start, end)
        chrom_sizes[chrom] = max(chrom_sizes[chrom], end)
    
    # 2. åŠ è½½é«˜äº®åŸºå›  (å®¶æ—æˆå‘˜)
    if self.highlights_file:
        for line in f:
            raw_id = line.strip()
            # IDå½’ä¸€åŒ–å¤„ç†
            if '_' in raw_id:
                parts = raw_id.rsplit('_', 1)
                if parts[1].isdigit():
                    normalized_id = f"{parts[0]}.{parts[1]}"  # XP_xxx_1 â†’ XP_xxx.1
                    highlight_ids.add(normalized_id)
    
    # 3. æŸ“è‰²ä½“é€‰æ‹©ç®—æ³•
    MAX_DISPLAY = 12
    
    # ä¼˜å…ˆä¿ç•™åŒ…å«ç›®æ ‡åŸºå› çš„æŸ“è‰²ä½“
    target_chroms = set()
    for gid in highlight_ids:
        if gid in gene_coords:
            target_chroms.add(gene_coords[gid][0])
    
    # é¢„æ‰«æanchorsç¡®å®šæœ‰é«˜äº®è¿æ¥çš„æŸ“è‰²ä½“
    for line in anchors_file:
        gene_a, gene_b = parts[0], parts[1]
        if gene_a in highlight_ids or gene_b in highlight_ids:
            chroms_with_highlight_links.add(gene_coords[gene_a][0])
            chroms_with_highlight_links.add(gene_coords[gene_b][0])
    
    # å¦‚æœç›®æ ‡æŸ“è‰²ä½“ < 12, ç”¨æœ€é•¿æŸ“è‰²ä½“å¡«å……
    if len(target_chroms) < MAX_DISPLAY:
        sorted_by_len = sorted(all_chroms, key=lambda k: chrom_sizes[k], reverse=True)
        for chrom in sorted_by_len:
            if chrom not in final_chrom_set:
                final_chrom_set.add(chrom)
                if len(final_chrom_set) >= MAX_DISPLAY: break
    
    # 4. ç”ŸæˆKaryotypeæ–‡ä»¶
    # æ ¼å¼: chr - chr_name label start end color
    for idx, chrom in enumerate(final_sorted):
        color = f"chr{idx % 20 + 1}"
        f.write(f"chr - {chrom} {chrom} 0 {chrom_sizes[chrom]} {color}\n")
    
    # 5. ç”ŸæˆLinksæ–‡ä»¶
    for line in anchors_file:
        gene_a, gene_b = parts[0], parts[1]
        chr_a, start_a, end_a = gene_coords[gene_a]
        chr_b, start_b, end_b = gene_coords[gene_b]
        
        # ä»…ç»˜åˆ¶é€‰å®šæŸ“è‰²ä½“é—´çš„è¿çº¿
        if chr_a in top_chrom_set and chr_b in top_chrom_set:
            # é«˜äº®è¿çº¿ (çº¢è‰²åŠ ç²—)
            if gene_a in highlight_ids or gene_b in highlight_ids:
                link_opts = "color=red,thickness=4,z=10"
            else:
                link_opts = "color=grey_a4"
            
            fout.write(f"{chr_a} {start_a} {end_a} {chr_b} {start_b} {end_b} {gene_a} {gene_b} {link_opts}\n")
```

#### 9.6 Step 5: Matplotlib Circosç»˜å›¾ (render_circos_plot)

```python
def _render_cartesian_circos(self, karyotype_path, links_path, output_path):
    import matplotlib.pyplot as plt
    import matplotlib.patches as patches
    import numpy as np
    
    # 1. å¸ƒå±€è®¡ç®—
    # å°†æŸ“è‰²ä½“æ’åˆ—æˆåœ†å½¢
    total_genome_len = sum(chrom_sizes.values())
    gap_angle = 2  # æŸ“è‰²ä½“é—´éš”è§’åº¦
    
    # 2. ç»˜åˆ¶æŸ“è‰²ä½“å¼§æ®µ
    for chrom, length in chroms:
        arc = patches.Arc(center, radius, radius, 
                         theta1=start_angle, theta2=end_angle)
        ax.add_patch(arc)
    
    # 3. ç»˜åˆ¶å…±çº¿æ€§è¿çº¿ (è´å¡å°”æ›²çº¿)
    for link in links:
        # è®¡ç®—èµ·ç‚¹å’Œç»ˆç‚¹çš„è§’åº¦ä½ç½®
        angle_a = get_angle(chr_a, pos_a)
        angle_b = get_angle(chr_b, pos_b)
        
        # é€šè¿‡ä¸­å¿ƒç‚¹ç»˜åˆ¶è´å¡å°”æ›²çº¿
        Path = mpath.Path
        path_data = [
            (Path.MOVETO, point_a),
            (Path.CURVE3, control_point),
            (Path.CURVE3, point_b)
        ]
        
        # é«˜äº®è¿çº¿ä½¿ç”¨çº¢è‰²
        if is_highlight:
            color = 'red'
            linewidth = 1.5
        else:
            color = 'gray'
            alpha = 0.3
    
    # 4. ä¿å­˜å›¾ç‰‡
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
```

---

## Phase 10: å¯åŠ¨å­åˆ†æ

### è„šæœ¬: `run_promoter_analysis.py` (147è¡Œ)

#### 10.1 å¯åŠ¨å­æ–‡ä»¶å®šä½

```python
def main():
    # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„å¯åŠ¨å­æ–‡ä»¶
    if args.promoter_file:
        promoter_file = Path(args.promoter_file)
    else:
        # å°è¯•ä»pipelineè¾“å‡ºç›®å½•æŸ¥æ‰¾
        candidates = list(final_dir.glob("*.promoter.fasta"))
        if candidates:
            promoter_file = candidates[0]
    
    # æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
    if promoter_file.stat().st_size == 0:
        logger.warning("Promoter file is empty. Skipping analysis.")
        sys.exit(0)
```

#### 10.2 Cis-elementæ•°æ®åº“å®šä½ (å¤šè·¯å¾„æœç´¢)

```python
# æœç´¢é¡ºåº:
# 1. config/cis_elements_default.txt
# 2. ./cis_elements_default.txt (å½“å‰ç›®å½•)
# 3. {script_dir}/cis_elements_default.txt (è„šæœ¬ç›®å½•)

cis_db_path = Path("config/cis_elements_default.txt")
if not cis_db_path.exists():
    cis_db_path = Path("cis_elements_default.txt")
if not cis_db_path.exists():
    script_dir = Path(__file__).parent.resolve()
    cis_db_path = script_dir / "cis_elements_default.txt"
```

#### 10.3 Golden Listæ‰«æ (æ­£åˆ™åŒ¹é…)

```python
# pipeline_utils.scan_promoters()
def scan_promoters(promoter_file, cis_db_path):
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰«æå·²çŸ¥çš„é¡ºå¼ä½œç”¨å…ƒä»¶
    
    cis_elements_default.txt æ ¼å¼:
    Element_Name\tSequence_Pattern
    ABRE\tACGTG[TC]
    W-box\tTTGAC[TC]
    MYB\tC[ACGT]GTT[AG]
    ...
    """
    cis_counts = {}  # Element â†’ Count
    cis_details = {} # Gene â†’ [(Element, Position, Sequence)]
    
    for gene_id, sequence in promoter_seqs:
        for element_name, pattern in cis_patterns:
            for match in re.finditer(pattern, sequence, re.IGNORECASE):
                cis_counts[element_name] += 1
                cis_details[gene_id].append({
                    'element': element_name,
                    'position': match.start(),
                    'sequence': match.group()
                })
    
    return cis_counts, cis_details
```

#### 10.4 De Novo Motifå‘ç° (MEME)

```python
# pipeline_utils.run_meme_promoter()
def run_meme_promoter(promoter_file, out_dir, threads=1):
    """
    å¯åŠ¨å­ä¸“ç”¨MEMEå‚æ•°
    """
    cmd = [
        "meme", str(promoter_file),
        "-dna",              # DNAæ¨¡å¼ (éè›‹ç™½)
        "-oc", str(out_dir),
        "-nmotifs", "10",    # å‘ç°10ä¸ªmotif
        "-minw", "6",        # æœ€å°å®½åº¦6bp
        "-maxw", "20",       # æœ€å¤§å®½åº¦20bp
        "-mod", "zoops",     # Zero Or One Per Sequence
        "-p", str(threads)   # å¹¶è¡Œçº¿ç¨‹
    ]
    
    subprocess.run(cmd, check=True)
```

#### 10.5 TOMTOM MotiféªŒè¯

```python
# pipeline_utils.run_tomtom()
def run_tomtom(meme_out_dir, jaspar_db_path):
    """
    å°†MEMEå‘ç°çš„motifä¸JASPARæ•°æ®åº“æ¯”å¯¹
    éªŒè¯å‘ç°çš„motifæ˜¯å¦å¯¹åº”å·²çŸ¥è½¬å½•å› å­ç»“åˆä½ç‚¹
    """
    meme_file = meme_out_dir / "meme.txt"
    tomtom_out = meme_out_dir / "tomtom_results"
    
    cmd = [
        "tomtom",
        "-oc", str(tomtom_out),
        "-evalue", "10.0",      # å®½æ¾é˜ˆå€¼ä¾¿äºæ¢ç´¢
        "-min-overlap", "5",    # æœ€å°é‡å 5bp
        str(meme_file),
        str(jaspar_db_path)
    ]
    
    subprocess.run(cmd, check=True)

# JASPARæ•°æ®åº“æ–‡ä»¶:
# cis_elements_jaspar_plants.meme (æ¤ç‰©è½¬å½•å› å­MEMEæ ¼å¼)
```

#### 10.6 ç»“æœè¾“å‡º

| æ–‡ä»¶ | å†…å®¹ | æ ¼å¼ |
|------|------|------|
| `cis_counts.json` | å…ƒä»¶è®¡æ•° | `{"ABRE": 45, "W-box": 32}` |
| `cis_details.json` | è¯¦ç»†å‘½ä¸­ | `{"Gene1": [{"element": "ABRE", "pos": 123}]}` |
| `MEME_Promoter/meme.txt` | MEMEè¾“å‡º | MEMEæ ‡å‡†æ ¼å¼ |
| `MEME_Promoter/tomtom_results/` | TOMTOMæ¯”å¯¹ | HTML + TSV |



---

## å…³é”®æŠ€æœ¯ç»†èŠ‚å¤‡å¿˜

### âš ï¸ å¿…é¡»æ³¨æ„çš„å¤„ç†æ­¥éª¤

| é˜¶æ®µ | å¤„ç† | å‚æ•° | åŸå›  |
|------|------|------|------|
| **ç§å­è·å–** | åºåˆ—å»é‡ | åŸºäºåºåˆ—å†…å®¹ | é¿å…å†—ä½™å½±å“HMM |
| **HMMæ„å»ºå‰** | é•¿åº¦è¿‡æ»¤ | 75%-125%ä¸­ä½æ•° | ç§»é™¤å¼‚å¸¸å€¼ |
| **HMMæ„å»ºå‰** | MSA Gapä¿®å‰ª | >90%åˆ—ç§»é™¤ | ä¼˜åŒ–HMM profile |
| **InterProæäº¤å‰** | åºåˆ—æ¸…æ´— | ç§»é™¤`*`å’Œ`.` | é¿å…APIé”™è¯¯ |
| **ç³»ç»Ÿå‘è‚²æ ‘å‰** | MSA Gapä¿®å‰ª | >50%åˆ—ç§»é™¤ | æé«˜æ ‘æ‹“æ‰‘è´¨é‡ |
| **å¯†ç å­æ¯”å¯¹å** | ç»ˆæ­¢å¯†ç å­ç§»é™¤ | TAA/TAG/TGA | é¿å…KaKsé”™è¯¯ |
| **å¯†ç å­æ¯”å¯¹å** | Nç¢±åŸºåˆ—ç§»é™¤ | å…¨éƒ¨ | ç¡®ä¿æ¯”å¯¹æœ‰æ•ˆ |
| **Motifåˆ†æ** | åŒºå—åˆå¹¶ | è·ç¦»<=10 | è¿æ¥ç›¸è¿‘ä¿å®ˆåŒº |

### ğŸ”§ é‡è¦é…ç½®å‚æ•°å‚è€ƒ

| å‚æ•° | é»˜è®¤å€¼ | è„šæœ¬ | è¯´æ˜ |
|------|--------|------|------|
| `evalue` | 1e-5 | search_extract | HMM/BLASTé˜ˆå€¼ |
| `max_gap_ratio` (HMM) | 0.9 | build_hmm | HMMæ„å»ºMSAä¿®å‰ª |
| `max_gap_ratio` (Tree) | 0.5 | pipeline_utils | ç³»ç»Ÿå‘è‚²MSAä¿®å‰ª |
| `conservation_threshold` | 0.7 | pipeline_utils | Motifä¿å®ˆæ€§é˜ˆå€¼ |
| `merge_distance` | 10 | pipeline_utils | MotifåŒºå—åˆå¹¶è·ç¦» |
| `batch_size` (CDD) | 400 | scan_cdd_ncbi | CDDåˆ†æ‰¹å¤§å° |
| `batch_size` (IPS) | 20 | interproscan_runner | InterProåˆ†æ‰¹å¤§å° |
| `MAX_CONCURRENT_JOBS` | 5 | interproscan_runner | InterProå¹¶å‘æ•° |
| `upstream_len` | 2000 | run_promoter_analysis | å¯åŠ¨å­é•¿åº¦ |
| `n_motifs` | 15 | pipeline_utils | MEMEå‘ç°æ•° |
| `bootstrap` | 1000 | pipeline_utils | IQ-TREE bootstrap |

### ğŸ”‘ IDæ˜ å°„ä¼˜å…ˆçº§

```
1. NCBI [protein_id=XP_xxx]          â†’ æœ€é«˜ä¼˜å…ˆçº§
2. NCBI lcl|..._prot_XP_xxx          â†’ æ¬¡ä¼˜å…ˆçº§
3. NCBI lcl|..._cds_XP_xxx           â†’ æ¬¡ä¼˜å…ˆçº§
4. Ensembl transcript:xxx             â†’ æ–°å¢(v3.1)
5. Ensembl gene:xxx                   â†’ ä¸­ä¼˜å…ˆçº§
6. EVM gene=xxx                       â†’ ä¸­ä¼˜å…ˆçº§
7. ç›´æ¥ä½¿ç”¨ record ID                 â†’ å…œåº•ç­–ç•¥
8. ç¬¬ä¸€ä¸ª token                       â†’ å¤‡ç”¨ç­–ç•¥
```

---

*æ–‡æ¡£ç»“æŸ - ç‰ˆæœ¬ v3.1*
